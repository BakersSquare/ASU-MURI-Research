{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdae610",
   "metadata": {},
   "source": [
    "Remember to 'pip install [package_name]' if these imports are not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fa413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports from selenium, request_futures, requests, bs4, pandas\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from requests_futures.sessions import FuturesSession\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Native python imports\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from concurrent.futures import as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c7394",
   "metadata": {},
   "source": [
    "# Enter Lat /Lon coordinates and Day of Interest\n",
    "These should be the only inputs needed before executing the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants for our current usecase\n",
    "day_of_interest = 51 # Day of Year (DOY) of February 20, which is our current examination\n",
    "g_truth_lat, g_truth_lon = 75.92, -149.69 # Selecting a coordinate of interest (EX: Center of SAR image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca336e5b",
   "metadata": {},
   "source": [
    "### Run this section to get references to the buoy data\n",
    "The buoy data is listed as a table of ids that is nested within an iframe of the website. Because of this iframe, we use selenium to simulate a web-client until the data loads, then grab it using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "base_url = f\"https://iabp.apl.uw.edu/IABP_Table.html\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "# executable_path param is not needed if you updated PATH\n",
    "browser = webdriver.Chrome(options=options)\n",
    "\n",
    "try:\n",
    "    browser.get(base_url)\n",
    "    timeout_in_seconds = 10\n",
    "    WebDriverWait(browser, timeout_in_seconds).until(ec.frame_to_be_available_and_switch_to_it(\"myframe\"))\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(\"Got the site!\")\n",
    "except TimeoutException:\n",
    "    print(\"I give up...\")\n",
    "finally:\n",
    "    browser.quit()\n",
    "    \n",
    "table = soup.find('table')\n",
    "tablebody = soup.find('tbody')\n",
    "tablerow = tablebody.find_all('tr')\n",
    "\n",
    "buoy_ids = []\n",
    "failed = 0\n",
    "for row in tablerow:\n",
    "    buoy_id = row.find('td')\n",
    "    try:\n",
    "        buoy_ids.append(buoy_id.get_text())\n",
    "    except:\n",
    "        failed += 1\n",
    "num_buoys = len(buoy_ids)\n",
    "print(f\"Buoys to search through: {num_buoys}\")\n",
    "print(f\"Rows without buoy ids: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a144120",
   "metadata": {},
   "source": [
    "### Run this next section to populate the buoy dictionary with distances\n",
    "Get all of the data from the different buoys, filter to only those that have record of our time of interest, and then calculate their distances from our point of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfeee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining url getter\n",
    "def get_url(url:str):\n",
    "    return f\"https://iabp.apl.uw.edu/WebData/{url}.dat\"\n",
    "\n",
    "regExStr = f'\\d{{5,20}}\\s*?2023(?:\\s*?\\d{{2}}){{2}}\\s*?(?:{day_of_interest}|{day_of_interest + 1}).*'\n",
    "\n",
    "# Declare our dictionary and counters\n",
    "distances_dict = {}\n",
    "num_completed = 0\n",
    "\n",
    "# Open a session to speed up the time it takes to make and receive all of our requests\n",
    "with FuturesSession() as session:\n",
    "    \n",
    "    # Get the url for each buoy id, and then get the data\n",
    "    urls = [get_url(id_num) for id_num in buoy_ids]\n",
    "    futures = [session.get(url) for url in urls]\n",
    "    \n",
    "    # Event listener to execute as the requests resolve\n",
    "    for future in as_completed(futures):\n",
    "        print(f\"{num_completed + 1} / {num_buoys} requests resolved\")\n",
    "        \n",
    "        # Convert from byte to string format\n",
    "        response = future.result().content.decode('UTF-8')\n",
    "        \n",
    "        # Get the column headers (and removing whitespace) using the split method\n",
    "        column_headers = response.partition('\\n')[0].split()\n",
    "        \n",
    "        # Find all rows of buoy data using regex\n",
    "        daily_data = re.findall(regExStr, response)\n",
    "        daily_data_len = len(daily_data)\n",
    "        \n",
    "        # Iterate through the data per buoy to calculate distance\n",
    "        if daily_data_len > 0:\n",
    "            buoy_id = daily_data[0].split()[0]\n",
    "            buoy_dist = 0\n",
    "            print(f\"Processing {num_completed+1}/{num_buoys} pages, with {daily_data_len} rows.\")\n",
    "            for row in daily_data:\n",
    "                try:\n",
    "                    # Calculating avg distance of buoy \n",
    "                    lat_index = column_headers.index(\"Lat\")\n",
    "                    lon_index = column_headers.index(\"Lon\")\n",
    "                    splits = row.split()\n",
    "                    lat = splits[lat_index]\n",
    "                    lon = splits[lon_index]\n",
    "                    seg_dist = math.dist([float(lat), float(lon)], [g_truth_lat, g_truth_lon])\n",
    "                    buoy_dist += seg_dist\n",
    "                except:\n",
    "                    print(\"Could not find lat/lon column for \", buoy_id)\n",
    "                    \n",
    "            # Set the distance in the dictionary\n",
    "            avg_dist = buoy_dist / daily_data_len\n",
    "            distances_dict[buoy_id] = avg_dist\n",
    "            print(f\"Finished processing {buoy_id}. Average Dist = {avg_dist}\")        \n",
    "        else:\n",
    "            print(f\"Request {num_completed + 1} has no matching data.\", )\n",
    "        num_completed += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bf9b4",
   "metadata": {},
   "source": [
    "### Run this section to extract the data from the nearest buoys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81269ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"Nearest5Buoys.csv\"\n",
    "num_buoys = 5\n",
    "\n",
    "df_cols = []\n",
    "closest_buoys = list(Counter(distances_dict).most_common()[-num_buoys:])\n",
    "closest_buoys.reverse()\n",
    "for b_id, dist in closest_buoys:\n",
    "\n",
    "    data_frame_data = []\n",
    "    response = get(get_url(b_id)).content.decode('UTF-8')\n",
    "    daily_data = re.findall(regExStr, response)\n",
    "    if len(daily_data) > 0:\n",
    "        column_headers = response.partition('\\n')[0].split()\n",
    "        # Make sure we have all column headers\n",
    "        for header in column_headers:\n",
    "            if header not in df_cols:\n",
    "                df_cols.append(header)\n",
    "        # Then push in the data so we can later create the df\n",
    "        for i, row in enumerate(daily_data):\n",
    "            splits = row.split()\n",
    "            data_frame_data.append(splits)\n",
    "\n",
    "# If the columns don't match up, we have pulled data from the optional columns and need to make sure they're in our headers\n",
    "data_csv = pd.DataFrame(data_frame_data, columns=column_headers)\n",
    "print(data_csv.head(5))\n",
    "print(data_csv.tail(5))\n",
    "\n",
    "# Comment / Uncomment this last line to toggle whether the file gets written\n",
    "data_csv.to_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca45ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
